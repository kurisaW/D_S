## 决策树

![image-20221014184605492](C:\Users\金鑫\AppData\Roaming\Typora\typora-user-images\image-20221014184605492.png)

#### 决策树的衡量标准

```
* 总结以上遇到的问题，归根结底就是什么特征能把数据划分的更好，选择哪个特征最好，就把它放在最前面。
* 用什么指标来衡量哪个最好？

熵是指物体内部的混乱程度，根据熵的定义，熵值越高，混乱程度越高。反之，数据划分我们也希望，同一类别聚集在一起，这样就达到了分类的目的。
```

#### 信息熵

信息熵的计算公式如下：

![image-20221014190825252](C:\Users\金鑫\AppData\Roaming\Typora\typora-user-images\image-20221014190825252.png)

熵（entropy）在统计学中是一个很重要的概念，用于特征的选择，衡量结果的不确定性， 信息熵越小， 结果越简单。

#### 条件熵
当一个特征固定以后， 结果的不确定性即为条件熵：

![img](https://upload-images.jianshu.io/upload_images/5223866-74b9fe2f226fb9db.png?imageMogr2/auto-orient/strip|imageView2/2/w/705/format/webp)

#### Gini系数

在构建分类决策树时，不仅可以使用熵值作为衡量标准，还可以使用Gin系数，原理基本一致，如下：

![image-20221014192747196](C:\Users\金鑫\AppData\Roaming\Typora\typora-user-images\image-20221014192747196.png)

D表示全样本， pi表示每种类别出现的概率, 极端情况p = 1 则Gini = 0 , 不纯度最低，最稳定。
类似的， Gini增益：

![img](https://upload-images.jianshu.io/upload_images/5223866-996e5ffe01cf29e1.png?imageMogr2/auto-orient/strip|imageView2/2/w/332/format/webp)

#### 信息增益
原熵-条件熵， 即为这个已确定变量给系统带来的稳定性增益：

![img](https://upload-images.jianshu.io/upload_images/5223866-344dbbe8235afdbb.png?imageMogr2/auto-orient/strip|imageView2/2/w/534/format/webp)

H(c) 是分类结果的gain

当然特征带来的稳定性增益越大越好。但是有个问题， 如果一个特征有非常多的value, 那么我们一个节点的分支也会对应很多，很容易造成过拟合

#### 信息增益比
 信息增益的一个大问题就是偏向选择分支多的属性导致overfitting，信息增益比公式对value值多的情况进行的惩罚处理(尽管如此，还是要剪枝)

![img](https://upload-images.jianshu.io/upload_images/5223866-6b74490e32d21640.png?imageMogr2/auto-orient/strip|imageView2/2/w/255/format/webp)

H(c) 是分类类别的熵， 类别越多，熵倾向于越大；同理， H(X)是变量X的Gain, 变量X的取值越多，H(X)的值也会越大， 惩罚除数越大：

![img](https://upload-images.jianshu.io/upload_images/5223866-4b90686e3e8e9c43.png?imageMogr2/auto-orient/strip|imageView2/2/w/289/format/webp)